{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bfc67f-67ec-44a8-be20-a0f1900e23cf",
   "metadata": {},
   "source": [
    "# Programming Model Exercises\n",
    "\n",
    "In this notebook, we will practice running a simple summation code in a number of different programming models. \n",
    "We will start by running it using a traditional CPU approach, before moving it onto the GPU and exploring \n",
    "\n",
    "Firstly, let's check we have suitable GPUs available with rocm-smi, move into the source directory and make sure the working environment is clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a55d7-8f13-4ebb-a939-50c91a6b596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocm-smi\n",
    "cd $HOME/DiRAC-AMD-GPU/notebooks/01-Programming_Model/1b-Programming_overview/source\n",
    "make clean\n",
    "make clean && rm -rf kokkos/build && rm -rf raja/build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfd0d1-b3ce-4d67-84ce-b4bf4ad3bfa9",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "You may want to take some time to familiarise yourself with the source code we will be using in this notebook before proceeeding.\n",
    "It is based on those used in the presentation, fleshed out into working code.\n",
    "\n",
    "It begins by allocating memory for two `double` arrays.\n",
    "The first array is initialised to contain `1` in every element, then each element of the second array is assigned twice the first array's value.\n",
    "Finally, the sum of the second array is calculated.\n",
    "\n",
    "Throughout this notebook we will look at how the different programming models can be applied to this code.\n",
    "Build instructions for all of the examples are contained in the directory's `Makefile`.\n",
    "\n",
    "Firstly, let's look at a traditional CPU approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91961077-f037-4c21-ad65-55241875e885",
   "metadata": {},
   "source": [
    "## CPU code baseline\n",
    "\n",
    "The standard CPU code is contained in the [`cpu_code.c`](./source/cpu_code.c) file. \n",
    "Its option in the [`Makefile`](./source/Makefile) is `cpu_code`, so let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82968a7-7ce1-41e9-aecc-46094a3a0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make cpu_code\n",
    "./cpu_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4853a42-5ca4-4620-bf24-90fe221dcb8b",
   "metadata": {},
   "source": [
    "Note that in this instance we are using the `amdclang` compiler, but this code will compile with any C (or C++) copmiler, and run on any CPU.\n",
    "\n",
    "The output gives us the expected result for an input array of length 100000: 200000.\n",
    "\n",
    "Let's now begin moving some of the code's execution to a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e067e7-fc52-4430-9ac9-ff51e57913b7",
   "metadata": {},
   "source": [
    "## Standard GPU code example\n",
    "\n",
    "The code contained in [`gpu_code.hip`](./source/gpu_code.hip) demonstrates a standard approach to running the code on a GPU using explicit memory management, i.e. moving the involved memory between the CPU and GPU ourselves.\n",
    "\n",
    "In this example, we begin by assigning the two arrays as before, but now we also have to assign the memory required on the GPU (which we normally refer to as the \"device\").\n",
    "This is done using the `hipMalloc` command on lines 36 and 37.\n",
    "The `hipMemcpy` command on lines 42 and 49 allows us to manually copy memory between the CPU (the host) and GPU (the device).\n",
    "The assignment of the second array is carried out on the GPU, before transferring the results back to calculate the final summation on the CPU.\n",
    "\n",
    "The code can be compiled using the `gpu_code` command.\n",
    "Let's compile and run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41589ca-b442-4e0a-bcd1-0f2e208eb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make ./gpu_code\n",
    "./gpu_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168e904-16d8-4f9a-8675-4c953c7cce6e",
   "metadata": {},
   "source": [
    "Note that we are now using the `hipcc` compiler in order to make use of the HIP commands in the code.\n",
    "\n",
    "This and future compilations will be given the `--offload-arch=${AMDGPU_GFXMODEL}` argument to tell the compiler what architecture of GPU to expect at runtime.\n",
    "For the MI200 series GPUs available on `COSMA` this string is `gfx90a`, but this can be changed to target different GPU architectures available elsewhere.\n",
    "It is possible to supply a semi-colon separated list in order to compile for multiple architectures at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1208552-54c5-4cad-b851-a8c8c91e45fa",
   "metadata": {},
   "source": [
    "## Managed memory code\n",
    "\n",
    "Our next example, [`gpu_code_managed.hip`](./source/gpu_code_managed.hip), demonstrates how this code can use managed memory, i.e. allowing the Operating System to move memory between the CPU and GPU for us.\n",
    "\n",
    "Comparing this to the previous example, we can see that the explicit HIP memory management calls - the `hipMalloc` and `hipMemcpy` commands discussed before - have been removed.\n",
    "These are not required when letting the OS handle the memory for us.\n",
    "\n",
    "This code is compiled using the `gpu_code_managed` option in the [`Makefile`](./source/Makefile).\n",
    "Let's compile the code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e4c1-5f5a-47e8-9399-2c2ef0f3a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "make gpu_code_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc30ac-9f74-443a-9658-98112fe73ca2",
   "metadata": {},
   "source": [
    "To tell the operating system to enable managed memory, we need to set the environment variable `HSA_XNACK` to 1.\n",
    "This instructions tells the GPU to retry memory accesses that fail due to a page fault, and migrate the memory automatically in such cases.\n",
    "This feature works for AMD GPUs of the MI200 and MI300 series, and can be disabled again by setting `HSA_XNACK=0`.\n",
    "\n",
    "Let's enable this now, and run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb42144-0332-4938-b93a-e776d8b56d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HSA_XNACK=1\n",
    "./gpu_code_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64d47d-c809-49b3-8511-cdb4123e2027",
   "metadata": {},
   "source": [
    "## OpenMP single address space\n",
    "\n",
    "We will now look at two examples of this loop offloading using OpenMP.\n",
    "The first, [`openmp_code.c`](./source/openmp_code.c), contains the offloaded loop within the `main` function, whilst the second, [`openmp_code1.c`](./source/openmp_code1.c), has these loops in a separate function where the compiler cannot tell the size of the array.\n",
    "Both contain the `omp requires unified_shared_memory` pragma and, as such, we might naively assume that they will not be able to run on our MI200 series GPUs, which do not have a unified shared memory.\n",
    "\n",
    "Let's nevertheless try compiling and running them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e279a-101f-4012-ba6b-79cd8e56dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "make openmp_code openmp_code1\n",
    "./openmp_code\n",
    "./openmp_code1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e2225-18fc-4492-8232-cc5f815755fe",
   "metadata": {},
   "source": [
    "Perhaps to our surprise, the code has compiled and run successfully.\n",
    "Could this mean that the code is running on the CPU only, since we don't have access to unified shared memory GPU?\n",
    "\n",
    "We can check this using the environment variable `LIBOMPTARGET_INFO`.\n",
    "This variable will report various information depending on the level requested.\n",
    "By setting `LIBOMPTARGET_INFO=1`, all data arguments passed to an OpenMP device kernel will be reported.\n",
    "If we are only running on the CPU, therefore, there will be no messages printed, but if we are successfully offloading to the GPU we will see reports from both loop pragmas.\n",
    "\n",
    "To turn off the messages, we can set `LIBOMPTARGET_INFO=0`.\n",
    "The different types of runtime information we can get from `LIBOMPTARGET_INFO` are [documented here](https://openmp.llvm.org/design/Runtimes.html#libomptarget-info) .\n",
    "\n",
    "Let's run it now and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f1d30-7a41-4669-8425-871fe0c0b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "export LIBOMPTARGET_INFO=1\n",
    "./openmp_code\n",
    "./openmp_code1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49ab0b-fcdb-4612-9630-490c4dc40b26",
   "metadata": {},
   "source": [
    "We can see from these results that the code is indeed being run on the GPU.\n",
    "The OpenMP compiler is smart enough to recognise the architecture we are using and allow managed memory even in an environment without the asked for single unified memory (you will learn more about how this work in the OpenMP course).\n",
    "\n",
    "Feel free to experiment with removing the various pragmas and running these again, to see how the reports change.\n",
    "\n",
    "So far we've looked at native and pragma-based approaches to GPU offloading.\n",
    "Now let's look at higher level performance portability frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d527c-c9dc-429c-ba5a-6ac635c6a5f9",
   "metadata": {},
   "source": [
    "## Higher level performance portability frameworks\n",
    "\n",
    "Here we will see how we can use two examples of expanded high level frameworks to create single-source application code that can run on both CPUs and GPUs.\n",
    "The above example code has been accelerated using the [Kokkos](https://github.com/kokkos/kokkos) and [RAJA](https://github.com/LLNL/RAJA) frameworks.\n",
    "\n",
    "To simplify their running in these notebooks, we have pre-installed and loaded these frameworks.\n",
    "Usually, they would be available on HPC systems through `modules` that can be loaded depending on user need.\n",
    "Where unavailable, they can also be built specifically within one's own userspace.\n",
    "\n",
    "### Kokkos\n",
    "\n",
    "Kokkos is a programming model developed by Sandia National Labs that uses a HIP backend to allow us to exploit the unique abilities of AMD GPUs in C++.\n",
    "\n",
    "Lets inspect the example [kokkos_code.cc](./source/kokkos/kokkos_code.cc). Note that in the code we have not had to declare the arrays in Kokkos Views.\n",
    "Instead, we will allow the OS to manage the memory by setting the `HSA_XNACK` environment variable to 1.\n",
    "\n",
    "We can compile (using CMake) and run the example code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c0732-bfdf-4046-aca6-1339aedfce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kokkos\n",
    "cmake -B build -DCMAKE_CXX_COMPILER=/opt/rocm/bin/hipcc\n",
    "cmake --build build\n",
    "export HSA_XNACK=1\n",
    "./build/kokkos_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1027411-4311-482e-b8dc-27c8d04e27e5",
   "metadata": {},
   "source": [
    "### RAJA\n",
    "\n",
    "RAJA is a C++ framework developed by the Lawrence Livermore National Lab.\n",
    "It is modular in structure, with separate compute and data management.\n",
    "It supports a range of GPUs including AMD systems, with key kernel patterns that have been optimised by AMD themselves.\n",
    "\n",
    "Lets inspect the example [raja_code.cc](./source/raja/raja_code.cc). Note that, similarly to the Kokkos example, we have only allocated the arrays on the host with malloc. We will therefore need to set the `HSA_XNACK` variable to 1.\n",
    "\n",
    "We can compile (using CMake) and run the example code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462425e1-ff13-470d-8dab-9c9788139254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../raja\n",
    "cmake -B build -DCMAKE_CXX_COMPILER=/opt/rocm/bin/hipcc\n",
    "cmake --build build\n",
    "export HSA_XNACK=1\n",
    "./build/raja_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69a760-8601-4794-a6cc-9742d857b3a9",
   "metadata": {},
   "source": [
    "Now that we've seen how various programming models can be implemented in a simple C code, we can start exploring more complicated constructs using different programming paradigms.\n",
    "\n",
    "The next chapter will discuss OpenMP, and how we can use it to get the most of our AMD GPUs and APUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NI200",
   "language": "bash",
   "name": "ni200"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
